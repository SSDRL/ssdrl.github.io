<!DOCTYPE html>
<html lang="en">

  <head>

    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <!-- <meta name="viewport" content="width=device-width, initial-scale=1"> -->
    <meta name="viewport" content="width=1024">

    <title>VLMNM Workshop @ ICRA 2024</title>

    <!-- <link rel="icon" type="image/png" href="images/robot_emoji.png"> -->

    <!-- bootstrap -->
    <link rel="stylesheet" href="./files/bootstrap.min.css">
    <link rel="stylesheet" href="./files/bootstrap-theme.min.css">
    <link rel="stylesheet" type="text/css" href="./files/style.css">

    <!-- Google fonts -->
    <link href="./files/google-fonts.css" rel="stylesheet" type="text/css">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-47054450-13"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-47054450-13');
    </script>

  </head>


  <body>

    <div class="container-fluid">

      <div class="row section">
        <div class="text-center">
          <h1 style="color:#443535">Vision-Language Models for Navigation and Manipulation (VLMNM)</h1>
          <br>
          <h4>Full-day hybrid workshop at <a href="https://2024.ieee-icra.org/">ICRA 2024</a>, Room 315</h4>
          <h4> </h4>
          <h4>Friday, May 17, 2024, <a href="https://greenwichmeantime.com/time/japan/" target="_blank">Japan Standard Time (JST)</a>, 
            <a href="https://2024.ieee-icra.org/yokohama/conference-venue/" target="_blank">Yokohama (Japan)</a></h4>
          <hr>
          <img src="photos/banner.jpg" width="100%"/>
          <hr>
          <!--h2><a href="https://openreview.net/group?id=IEEE.org/2024/ICRA/Workshop/VLMNM" target="_blank" style="color: #e74c3c !important;">
            Submit via OpenReview
          </a></h2-->
  
        </div>
      </div>

    <!--/div>

      <div class="row section"-->
        <!--center><h3>Introduction</h3><hr></center-->

        With the rising capabilities of LLMs and VLMs, the past two years have seen a surge in research work using VLMs for navigation and manipulation.
        Fusing the capabilities of visual interpretation with natural language processing, these models are poised to redefine how robotic systems interact
        with both their environment and human counterparts. The relevance of this topic cannot be overstated; as the frontier of human-robot interaction expands,
        so does the necessity for robots to comprehend and operate within complex environments using naturalistic instructions. Our workshop will not only reflect
        the state-of-the-art advancements in this domain, by featuring a diverse set of speakers, from senior academics to researchers in early careers, from industry
        researchers to companies producing mobile manipulation platforms, from researchers who are enthusiastic about using VLMs for robotics to those who have
        reservations about it. We aim for this event to be a catalyst for originality and diversity at ICRA 2024. We believe that, amidst a sea of workshops, ours
        will provide unique perspectives that will push the boundaries of what's achievable in robot navigation and manipulation.
        <br><br>
        In this workshop, we plan to discuss:
        <ul>
          <li>How can VLMs/LLMs enhance robotics navigation and manipulation?</li>
          <li>How to extract world knowledge from pre-trained VLMs/LLMs and apply them to navigation and manipulation?</li>
          <li>How to integrate VLMs/LLMs with robot components, such as perception, control, and planning? How to account for partial observability and uncertainty?</li>
          <li>Benchmarks and datasets to assess the generalization capabilities of VLMs/LLMs for navigation and manipulation.</li>
          <li>Capabilities and limitations of VLMs/LLMs for navigation and manipulation (e.g. in task planning, spatial understanding)</li>
          <li>New interaction modes between robots and humans enabled by VLMs/LLMs.</li>
        </ul>

        All accepted workshop papers: <a href="https://openreview.net/group?id=IEEE.org/2024/ICRA/Workshop/VLMNM#tab-accept" target="_blank">OpenReview</a>. Please bring a poster. We will have double-sided A0 portrait poster boards.

      </div>

      <div class="row section">
        <center><h3>Final Schedule</h3><hr></center>
        <div class="text-justify">
          <table>
            <thead>
              <tr class="highlight">
                <th class="time">Time (<a href="https://greenwichmeantime.com/time/japan/" target="_blank">JST</a>)</th>
                <th class="event">Event</th>
                <th class="description" style="width:12%">Description</th>
                <th class="time">Time (<a href="https://greenwichmeantime.com/time-zone/usa/pacific-daylight-time/" target="_blank">PDT</a>) <br>(May 16)</th>
              </tr>
            </thead>

            <tbody>
              <tr>
                <td class="time">8:30 - 8:50</td>
                <td class="event">Coffee and Pasteries<br>Poster presenters set up posters</td>
                <td class="description">
                    <b></b>
                </td>
                <td class="time">15:30 - 15:50</td>
              </tr>

              <tr class="highlight">
                <td class="time">8:50 - 9:00</td>
                <td class="event">
                  Introduction
                </td>
                <td class="description">
                </td>
                <td class="time">15:50 - 16:00</td>
              </tr>

              <tr>
                <td class="time">9:00 - 9:20</td>
                <td class="event">
                  LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks
                  <br><br><a href="https://search.asu.edu/profile/95646" target="_blank">Prof. Subbarao Kambhampati</a>
                  | Arizona State University
                </td>
                <td class="description">
                  <a href="https://search.asu.edu/profile/95646" target="_blank"><img src="photos/Speakers/SubbaraoKambhampati.png" width="100px" align="bottom" style="border-radius: 50%"></a>
                </td>
                <td class="time">16:00 - 16:20</td>
              </tr>

              <tr class="highlight">
                <td class="time">9:20 - 9:40</td>
                <td class="event">
                  LLM-based Task and Motion Planning for Robots
                  <br><br><a href="https://chuchu.mit.edu/" target="_blank">Prof. Chuchu Fan</a>
                  | Massachusetts Institute of Technology
                </td>
                <td class="description">
                  <a href="https://chuchu.mit.edu/" target="_blank"><img src="photos/Speakers/ChuchuFan.jpeg" width="100px" align="bottom" style="border-radius: 50%"></a>
                </td>
                <td class="time">16:20 - 16:40</td>
              </tr>

              <tr>
                <td class="time">9:40 - 10:00</td>
                <td class="event">
                  BEHAVIOR Vision Suite: Customizable Dataset Generation via Simulation
                  <br><br><a href="https://ai.stanford.edu/~zharu/" target="_blank">Dr. Ruohan Zhang</a>
                  | Stanford University
                </td>
                <td class="description">
                  <a href="https://ai.stanford.edu/~zharu/" target="_blank"><img src="photos/Speakers/RuohanZhang.png" width="100px" align="bottom" style="border-radius: 50%"></a>
                </td>
                <td class="time">16:40 - 17:00</td>
              </tr>

              <tr class="highlight">
                <td class="time">10:00 - 10:30</td>
                <td class="event">
                  Coffee Break and Poster Session
                </td>
                <td class="description">
                  <b>30 Mins</b>
                </td>
                <td class="time">17:00 - 17:30</td>
              </tr>

              <tr>
                <td class="time">10:30 - 10:50</td>
                <td class="event">
                  LLM-State: Adaptive State Representation for Long-Horizon Task Planning in the Open World
                  <br><br><a href="https://www.comp.nus.edu.sg/~dyhsu/" target="_blank">Prof. David Hsu</a>
                  | National University of Singapore
                </td>
                <td class="description">
                  <a href="https://www.comp.nus.edu.sg/~dyhsu/" target="_blank"><img src="photos/Speakers/DavidHsu.jpeg" width="100px" align="bottom" style="border-radius: 50%"></a>
                </td>
                <td class="time">17:30 - 17:50</td>
              </tr>

              <tr class="highlight">
                <td class="time">10:50 - 11:10</td>
                <td class="event">
                  LLM for Task Generation and Feedback
                  <br><br><a href="https://www.cs.utexas.edu/~yukez/" target="_blank">Prof. Yuke Zhu</a>
                  | University of Texas at Austin
                </td>
                <td class="description">
                  <a href="https://www.cs.utexas.edu/~yukez/" target="_blank"><img src="photos/Speakers/YukeZhu.jpg" width="100px" align="bottom" style="border-radius: 50%"></a>
                </td>
                <td class="time">17:50 - 18:10</td>
              </tr>

              <tr>
                <td class="time">11:10 - 11:30</td>
                <td colspan="2" class="event">
                  <table style="width: auto; margin-bottom: 0px;">
                    <tr>
                      <td style="width: 49%; padding: 0px;">
                        Demo: a Chat with Kachaka, a Home Robot 
                        <br><br><a href="https://www.linkedin.com/in/takafumi-watanabe/" target="_blank">Takafumi Watanabe</a>, <a href="https://www.linkedin.com/in/kenichi-hidai/?locale=ja_JP" target="_blank">Kenichi Hidai</a>
                        <br> | <a href="https://www.preferred.jp/en/projects/personal-robot/" target="_blank">Preferred Robotics Inc.</a>
                      </td>      
                      <td style="width: 35%;">
                        <a href="https://www.linkedin.com/in/takafumi-watanabe/" target="_blank"><img src="photos/Speakers/TakafumiWatanabe.png" width="100px" align="bottom" style="border-radius: 50%"></a>
                        <a href="https://www.linkedin.com/in/kenichi-hidai/?locale=ja_JP" target="_blank"><img src="photos/Speakers/KenichiHidai.jpg" width="100px" align="bottom" style="border-radius: 50%"></a>
                      </td>                                  
                    </tr>
                  </table>
                </td>
                <td class="time">18:10 - 18:30</td>
              </tr>

              <tr class="highlight">
                <td class="time">11:30 - 12:00</td>
                <td colspan="2" class="event">
                  <b>Panel: Bridging the Gap between Research & Industry</b>
                  <br>
                  Moderator: <a href="https://www.microsoft.com/en-us/research/people/nawake/" target="_blank">Naoki Wake</a>, Microsoft Research
                  <br>
                  <table style="width: auto; margin-bottom: -20px;">
                    <tr>
                      <td style="width: 24%; vertical-align: top; padding:10px">
                        <a href="https://cpaxton.github.io/about/" target="_blank"><img src="photos/Organizers/ChrisPaxton.jpeg" width="100px" align="bottom" style="border-radius: 50%"></a>
                        <br><p style="padding-top: 10px;"><a href="https://cpaxton.github.io/about/" target="_blank">Chris Paxton</a>
                        <br>| <a href="https://hello-robot.com/" target="_blank">Hello Robot</a></p>
                      </td>
                      <td style="width: 24%; vertical-align: top; padding:10px">
                        <a href="https://www.linkedin.com/in/takafumi-watanabe/" target="_blank"><img src="photos/Speakers/TakafumiWatanabe.png" width="100px" align="bottom" style="border-radius: 50%"></a>
                        <br><p style="padding-top: 10px;"><a href="https://www.linkedin.com/in/takafumi-watanabe/" target="_blank">Takafumi Watanabe</a>
                        <br>| <a href="https://www.preferred.jp/en/projects/personal-robot/" target="_blank">Preferred Robotics Inc.</a></p>
                      </td>
                      <td style="width: 24%; vertical-align: top; padding:10px">
                        <a href="https://mohitshridhar.com/" target="_blank"><img src="photos/Speakers/MohitShridhar.png" width="100px" align="bottom" style="border-radius: 50%"></a>
                        <br><p style="padding-top: 10px;"><a href="https://mohitshridhar.com/" target="_blank">Dr. Mohit Shridhar</a>
                        <br>| <a href="https://www.youtube.com/watch?v=JYFYLKEinlk&ab_channel=Dyson" target="_blank">Dyson Robot Learning Lab</a></p>
                      </td>
                      <td style="width: 24%; vertical-align: top; padding:10px">
                        <a href="https://www.lerrelpinto.com/" target="_blank"><img src="photos/Speakers/LerrelPinto.jpg" width="100px" align="bottom" style="border-radius: 50%"></a>
                        <br><p style="padding-top: 10px;"><a href="https://www.lerrelpinto.com/" target="_blank">Prof. Lerrel Pinto</a>
                        <br>| NYU Courant</p>
                      </td>                      
                    </tr>
                  </table>
                </td>
              
                <td class="time">18:30 - 19:00</td>
              </tr>

              <tr>
                <td class="time">12:00 - 13:30</td>
                <td class="event">
                  Lunch Break
                </td>
                <td class="description">
                  <b>1.5 Hours</b>
                </td>
                <td class="time">19:00 - 20:30</td>
              </tr>

              <tr class="highlight">
                <td class="time">13:30 - 13:50</td>
                <td class="event">
                  Language as Bridge for Sim2Real
                  <br><br><a href="https://robertomartinmartin.com/" target="_blank">Prof. Roberto Martín-Martín</a>
                  | University of Texas at Austin
                </td>
                <td class="description">
                  <a href="https://robertomartinmartin.com/" target="_blank"><img src="photos/Speakers/RobertoMartinMartin.jpeg" width="100px" align="bottom" style="border-radius: 50%"></a>
                </td>
                <td class="time">20:30 - 20:50</td>
              </tr>

              <tr>
                <td class="time">13:50 - 14:10</td>
                <td class="event">
                  Foundation Models of and for Navigation
                  <br><br><a href="https://people.eecs.berkeley.edu/~shah/" target="_blank">Dhruv Shah</a>
                  | University of California, Berkeley
                </td>
                <td class="description">
                  <a href="https://people.eecs.berkeley.edu/~shah/" target="_blank"><img src="photos/Speakers/DhruvShah.jpg" width="100px" align="bottom" style="border-radius: 50%"></a>
                </td>
                <td class="time">20:50 - 21:10</td>
              </tr>

              <tr class="highlight">
                <td class="time">14:10 - 14:30</td>
                <td class="event">
                  Mobile Manipulation, Multi-Agent Coordination, Long Horizon Tasks
                  <br><br><a href="https://web.stanford.edu/~bohg/" target="_blank">Prof. Jeannette Bohg</a>
                  | Stanford University
                </td>
                <td class="description">
                  <a href="https://web.stanford.edu/~bohg/" target="_blank"><img src="photos/Speakers/JeannetteBohg.png" width="100px" align="bottom" style="border-radius: 50%"></a>
                </td>
                <td class="time">21:10 - 21:30</td>
              </tr>

              <tr>
                <td class="time">14:30 - 15:00</td>
                <td colspan="2" class="event">
                  <b>Spotlight Talks (six)</b>
                  
                  <ul style="font-size: 12px; margin: 0; padding: 0; padding-top: 10px;">
                    <li>RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for Robotic Manipulation</li>
                    <li>MOSAIC: A Modular System for Assistive and Interactive Cooking</li>
                    <li>CoPa: General Robotic Manipulation through Spatial Constraints of Parts with Foundation Models</li>
                    <li>Deploying and Evaluating LLMs to Program Service Mobile Robots</li>
                    <li>Language-Grounded Dynamic Scene Graphs for Interactive Object Search with Mobile Manipulation</li>
                    <li>Language Models as Zero-Shot Trajectory Generators</li>
                  </ul>
                  
                </td>
                <td class="time">21:30 - 22:00</td>
              </tr>

              <tr class="highlight">
                <td class="time">15:00 - 16:00</td>
                <td class="event">
                  Coffee Break and a Longer Poster Session
                </td>
                <td class="description">
                  <b>1 Hour</b>
                </td>
                <td class="time">22:00 - 23:00</td>
              </tr>

              <tr>
                <td class="time">16:00 - 16:40</td>
                <td colspan="2" class="event">
                  <b>Debate: Is Large Foundation Models the most important research topic in the next 5 years? And various other questions.</b>
                  <br>
                  Participants: Roberto Martín-Martín, Dhruv Shah, and various other speakers.
                  <br>
                  Moderator: <a href="https://mahis.life/" target="_blank">Nur Muhammad Mahi Shafiullah</a>, New York University
                </td>
                <td class="time">23:00 - 23:40</td>
              </tr>

              <tr class="highlight">
                <td class="time">16:40 - 16:55</td>
                <td class="event">
                  <b>Moderated Open Discussion:</b> What’s Down the Horizon? / The 1 Billion Dollar Proposal
                </td>
                <td class="description">
                  <b>All in-person attendees</b>
                </td>
                <td class="time">23:40 - 23:55</td>
              </tr>

              <tr>
                <td class="time">16:55 - 17:00</td>
                <td class="event">
                  Best Paper Awards Ceremony and Closing Remarks
                </td>
                <td class="description">
                </td>
                <td class="time">23:55 - 00:00</td>
              </tr>
              
              <tr class="highlight">
                <th class="time"><b>&uarr; Time (<a href="https://greenwichmeantime.com/time/japan/" target="_blank">JST</a>)</b></th>
                <th class="event"><b>&uarr; Event</b></th>
                <th class="description"><b></b></th>
                <th class="time"><b>&uarr; Time (<a href="https://greenwichmeantime.com/time-zone/usa/pacific-daylight-time/" target="_blank">PDT</a>) <br>(1 Day Earlier)</b></th>
              </tr>
              
            </tbody>
          </table>
        </div>
      </div>

      <div class="row section">
        <center><h3>FAQ</h3><hr></center>
        <div class="faq-container">
          <div class="faq">
              <div class="faq-question" onclick="toggleAnswer('answer1')">Are you going to record the talks and post them later on YouTube?</div>
              <div class="faq-answer" id="answer1">We’re going to post the talks of speakers who permit us to on YouTube. But we will NOT post the recordings of the panel discussion, the debate, or the open discussion at the end.</div>
          </div>
          <div class="faq">
              <div class="faq-question" onclick="toggleAnswer('answer2')">Can I present remotely if my paper is accepted as a poster or a spotlight talk?</div>
              <div class="faq-answer" id="answer2">We will play a pre-recording of your spotlight talk and we will strongly encourage you to find friends to help present the poster in person.</div>
          </div>
          <!-- Add more FAQs here -->
        </div>
      </div>

      <div class="row section">
        <center><h3>Call for Papers</h3><hr></center>
          We invite submissions including but not limited to the following topics:
          <ul>
            <li>Applications:</li>
            <ul>
              <li>Integration of VLM/LLMs for manipulation and navigation</li>
              <li>VLM/LLMs for perception/scene understanding/state estimation</li>
              <li>VLM/LLMs for control/skill learning/motion generation</li>
              <li>VLM/LLMs for decision-making/reasoning/planning</li>
              <li>VLM/LLMs as world models</li>
              <li>VLMs/LLMs for multimodal task specifications</li>
              <li>VLMs/LLMs for human-robot/robot-robot interactions</li>
              <li>VLMs/LLMs for scene and task generation</li>
            </ul>
            <li>New Capabilities:</li>
            <ul>
              <li>Open-vocabulary perception/navigation/manipulation</li>
              <li>Commonsense reasoning with VLM/LLMs</li>
              <li>Generalization to unseen object categories, environments, and tasks</li>
              <li>Bootstrapping learning from scarce data</li>
              <li>Natural language interaction with everyday users</li>
            </ul>
            <li>Datasets/Benchmarks:</li>
            <ul>
              <li>Internet-scale data for training robotics foundation models</li>
              <li>Mobile manipulation benchmarks for VLM/LLM-based systems</li>
            </ul>
            <li>Limitations:</li>
            <ul>
              <li>Failure modes of VLM/LLMs</li>
              <li>Robustness of VLM/LLMs</li>
              <li>Certifiabilities of VLM/LLMs</li>
            </ul>
          </ul>
          
          Submissions should have up to 4 pages of technical content, with no limit on references/appendices. Submissions are suggested to follow the ICRA double-column format with the template available <b><a href="https://ras.papercept.net/conferences/support/support.php" target="_blank">here</a></b>. 
          We encourage authors to upload videos, code, or data as supplementary material (due on the same day as the paper). Following the main conference, our workshop will use a <b>single-blind</b> review process. 
          We welcome both unpublished, original contributions and recently published relevant works.
          
          Accepted papers will be presented as posters or orals and made public via the workshop’s OpenReview page with the authors’ consent. 
          We strongly encourage at least one of the authors to <b>present on-site</b> during the workshop. Our workshop will feature a Best Paper Award. <br>
          
          Important Dates:
          <ul>
            <li>Submission portal opens: <s>January 29, 2024</s></li>
            <li>Paper submission deadline: 
              <s>March 11, Monday, 2024 (AoE) </s>
                <!-- - Countdown: 
                  <span id="days"></span>
                  <span id="hours"></span>
                  <span id="mins"></span>
                  <span id="secs"></span>
                  <span id="end"></span> -->
            </li>
            <li>Notification of acceptance: <s>March 29, 2024 (Results viewable on <b><a href="https://openreview.net/group?id=IEEE.org/2024/ICRA/Workshop/VLMNM" target="_blank">OpenReview</a></b>)</s> April 1, 2024 (Announcing Spotlights)</li>
            <li>Camera-ready deadline: <s>April 26, 2024</s></li>
            <li>Workshop @ ICRA 2024: May 17, 2024</li>
          </ul>

      </div>
        
      <div class="row section">
        <center><h3>Organizers</h3><hr></center>

        <table>
          <tbody>
            <tr>
              <td width="25%" class="organizer">
                <a href="https://cpaxton.github.io/about/" target="_blank"><img src="photos/Organizers/ChrisPaxton.jpeg" width="150px" align="bottom" style="border-radius: 50%"></a>
                <br><br>
                <a href="https://cpaxton.github.io/about/" target="_blank">Chris Paxton</a>
                <br>
                FAIR, Meta
              </td>

              <td width="25%" class="organizer">
                <a href="https://fxia22.github.io/" target="_blank"><img src="photos/Organizers/FeiXia.jpeg" width="150px" align="bottom" style="border-radius: 50%"></a>
                <br><br>
                <a href="https://fxia22.github.io/" target="_blank">Fei Xia</a>
                <br>
                Google Deepmind
              </td>

              <td width="25%" class="organizer">
                <a href="http://karmeshyadav.com/" target="_blank"><img src="photos/Organizers/KarmeshYadav.jpeg" width="150px" align="bottom" style="border-radius: 50%"></a>
                <br><br>
                <a href="http://karmeshyadav.com/" target="_blank">Karmesh Yadav</a>
                <br>
                Georgia Tech
              </td>

              <td width="25%" class="organizer">
                <a href="https://mahis.life/" target="_blank"><img src="photos/Organizers/MahiShafiullah.jpeg" width="150px" align="bottom" style="border-radius: 50%"></a>
                <br><br>
                <a href="https://mahis.life/" target="_blank">Nur Muhammad Mahi Shafiullah</a>
                <br>
                New York University
              </td>
            </tr>

            <tr>
              <td width="25%" class="organizer">
                <a href="https://www.microsoft.com/en-us/research/people/nawake/" target="_blank"><img src="photos/Organizers/NaokiWake.jpeg" width="150px" align="bottom" style="border-radius: 50%"></a>
                <br><br>
                <a href="https://www.microsoft.com/en-us/research/people/nawake/" target="_blank">Naoki Wake</a>
                <br>
                Microsoft Research
              </td>

              <td width="25%" class="organizer">
                <a href="http://weiyuliu.com/" target="_blank"><img src="photos/Organizers/WeiyuLiu.jpg" width="150px" align="bottom" style="border-radius: 50%"></a>
                <br><br>
                <a href="http://weiyuliu.com/" target="_blank">Weiyu Liu</a>
                <br>
                Stanford University
              </td>

              <td width="25%" class="organizer">
                <a href="https://research.google/people/107814/" target="_blank"><img src="photos/Organizers/YujinTang.jpeg" width="150px" align="bottom" style="border-radius: 50%"></a>
                <br><br>
                <a href="https://research.google/people/107814/" target="_blank">Yujin Tang</a>
                <br>
                Sakana AI
              </td>

              <td width="25%" class="organizer">
                <a href="https://zt-yang.com/ " target="_blank"><img src="photos/Organizers/ZhutianYang.jpg" width="150px" align="bottom" style="border-radius: 50%"></a>
                <br><br>
                <a href="https://zt-yang.com/ " target="_blank">Zhutian Yang</a>
                <br>
                MIT, NVIDIA Research
              </td>

            </tr>
          </tbody>
        </table>
      </div>

      <div class="row section">
        <center><h3>Contact</h3><hr></center>
        For further information or questions, please contact <b>vlm-navigation-manipulation-workshop [AT] googlegroups [DOT] com</b>
      </div>

      <div class="row section">
        <center><h3>Access Map</h3><hr></center>
        <div class="stat">
          <script type='text/javascript' id='mapmyvisitors' src='https://mapmyvisitors.com/map.js?cl=ffe9c1&w=200&t=n&d=3Z2laR6dYlF9c9PU6rsM6lRNSGaZ1Qe9vDz5V5E0QPs&co=8ed0ff'></script>
        </div>
      </div>


  <script src="files/script.js"></script>
  </body>

</html>
